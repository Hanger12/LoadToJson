# Парсер цитат с сайта "Quotes to Scrape"

## Описание
Этот проект представляет собой асинхронный парсер, написанный на Python, для сбора цитат, авторов и тегов с сайта [Quotes to Scrape](http://quotes.toscrape.com/). Парсер использует библиотеки `aiohttp` для асинхронных запросов и `BeautifulSoup` для парсинга HTML-контента. Данные собираются со всех страниц сайта, включая страницы с пагинацией. Собранные данные сохраняются в формате JSON.

## Что было сделано

1. **Загрузка страниц**: Парсер асинхронно загружает HTML-контент с указанных страниц с использованием библиотеки `aiohttp`.
2. **Парсинг данных**: С помощью `BeautifulSoup` извлекаются цитаты, авторы и теги с каждой страницы.
3. **Пагинация**: Если на сайте предусмотрены страницы с пагинацией, парсер автоматически переходит к следующей странице и продолжает сбор данных.
4. **Сохранение данных**: Собранные данные сохраняются в формате JSON.

## Откуда были получены данные

Данные были получены с сайта [Quotes to Scrape](http://quotes.toscrape.com/). В конфигурационном файле `config.yaml` указаны URL сайта и правила парсинга для извлечения цитат, авторов и тегов, а также селектор для перехода на следующую страницу.

## Как осуществляется сбор данных

1. **Конфигурация**: Все настройки, включая URL-адреса сайтов и селекторы для парсинга, задаются в конфигурационном файле `config.yaml`.
2. **Загрузка HTML**: Для каждого сайта парсер загружает HTML-страницу с помощью асинхронных запросов через `aiohttp`.
3. **Парсинг**: С помощью `BeautifulSoup` извлекаются цитаты, авторы и теги с каждой страницы.
4. **Пагинация**: Если на странице имеется ссылка на следующую страницу (с использованием селектора для пагинации), парсер продолжает собирать данные с новой страницы.
5. **Сохранение данных**: После сбора данных они сохраняются в файл JSON.

## Почему были выбраны эти методы и инструменты

- **Асинхронность**: Использование `asyncio` и `aiohttp` позволяет эффективно загружать страницы без блокировки выполнения программы, что ускоряет процесс парсинга, особенно при большом количестве страниц.
- **BeautifulSoup**: Эта библиотека удобна и гибка для извлечения данных из HTML, даже если структура страниц не всегда идеальна.
- **JSON**: Формат JSON выбран для удобства хранения и обмена данными, так как он легко читается как человеком, так и программами.

## Установка и использование
### Шаг 1: Клонирование репозитория

Сначала клонируйте репозиторий с кодом на ваш локальный компьютер:

```bash
git clone https://github.com/Hanger12/LoadToJson.git
cd LoadToJson
```
### Шаг 2: Создание и активация виртуального окружения

Создайте виртуальное окружение для установки зависимостей, чтобы изолировать их от глобальной системы:
```bash
python -m venv .venv
```

Активация на Windows:
```bash
.venv\Scripts\activate
```

Активация на MacOS и Linux:
```bash
source venv/bin/activate
```
### Шаг 3: Установка зависимостей

Для установки зависимостей выполните следующую команду:

```bash
pip install -r requirements.txt
```

### Шаг 4: Запуск Парсера.
Для запуска парсера выполните следующую команду:
```bash
python parser.py
```
### Шаг 5: Просмотр результата.
Собранные данные сохраняются в файл `collected_data.json` в корневой папке проекта.
